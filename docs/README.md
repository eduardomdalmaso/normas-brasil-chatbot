# Normas Brasil Chatbot ğŸ¤–

An interactive assistant built with **Streamlit** to query Brazilian technical standards (NRs) from PDF or HTML files.  
This project allows professionals and students to ask questions about any regulation (e.g., NRâ€‘33, NRâ€‘35) and get contextual answers directly from the document.

---

## ğŸš€ Purpose
The goal of this project is to:
- Provide **fast access** to Brazilian technical standards.  
- Allow professionals in occupational safety, engineers, and students to make **intelligent queries** without manually reading entire documents.  
- Serve as a **support tool** in training, audits, and studies.  

---

## âš™ï¸ How it works
1. The user provides a **PDF or HTML file** containing the regulation.  
2. The text is split into **chunks** and converted into **embeddings**.  
3. The system uses **FAISS** for semantic search.  
4. The language model receives the most relevant chunks (`top_k`) and generates a contextualized answer.  
5. The **Streamlit interface** simulates a chat for natural interaction.  

---

## ğŸ–¥ï¸ System Requirements
Running large models on **CPU** requires many resources (mistralai/Mistral-7B-Instruct-v0.2).  
During tests, this project consumed **~40 GB of RAM** on CPU to load and execute a 7B parameter model.

### CPU vs GPU
- **CPU**:  
  - Slower for parallel computations.  
  - High RAM usage (~40 GB for 7B models).  
  - Longer response times.  

- **GPU**:  
  - Uses **VRAM**, optimized for matrix operations.  
  - The same model can run in **8â€“16 GB VRAM**, depending on quantization.  
  - Much faster responses.  

ğŸ‘‰ Summary: **CPU = accessible but heavy**, **GPU = optimized and fast**.

---

## ğŸ“¦ Installation
Clone the repository and install dependencies:

```bash
git clone https://github.com/eduardomdalmaso/normas-brasil-chatbot.git
cd normas-brasil-chatbot
pip install -r env/requirements.txt
```
---

### â–¶ï¸ Usage

**Pipeline scripts**

The project includes a pipeline to process documents and run the assistant:

1. python src/preprocessing.py   # Cleans and prepares the text from PDF/HTML
2. python src/embeddings.py      # Generates embeddings and builds FAISS index
3. bash src/run.sh               # Launches Streamlit app with the model

Running everything at once

You can also run the full pipeline with:
python run_pipeline.py

This will execute:

    Preprocessing

    Embeddings generation

    Streamlit app launch

---

## ğŸ“‚ Project Structure
```
emb_nrs/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ embeddings          # embedding for model
â”‚   â”œâ”€â”€ processed           # processed data
â”‚   â”œâ”€â”€ raw                 # .html + .pdf data to clean+process
â”‚
â”œâ”€â”€ logs                    # saved answers
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create_structure.sh # Handles conda enviroment
â”‚   â”œâ”€â”€ run_pipeline.py     # Run model only without web interface
â”‚   â”œâ”€â”€ run.sh              # Run full project
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing.py   # Handles PDF/HTML parsing and cleaning
â”‚   â”œâ”€â”€ embeddings.py      # Generates embeddings and FAISS index
â”‚   â”œâ”€â”€ qa.py              # Question-answer logic
â”‚   â””â”€â”€ search.py          # Embedding + FAISS gen
â”‚
â”œâ”€â”€ streamlit_app/
â”‚   â”œâ”€â”€ app.py             # Streamlit app

```

---

## ğŸŒŸ Future Improvements

GPU optimization for lower memory usage.

Highlighting the exact text chunks used in answers.

---

## ğŸ“œ License
This project is licensed under the MIT License. You are free to use, modify, and distribute it for educational and professional purposes. See the LICENSE file for details.
